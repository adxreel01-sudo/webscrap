ğŸ¯ GOAL FOR SUBMISSION (VERY CLEAR)

By the end of 5â€“6 hours, you should be able to confidently demo:

User enters a company website â†’ auto-filled company info

User clicks â€œScrape Productsâ€ â†’ products are scraped & stored

Weekly auto-update â†’ cron exists and works

Code looks structured, scalable, and production-minded

Not â€œperfect scrapingâ€, but excellent engineering thinking.

ğŸ§± MILESTONE PLAN (5â€“6 HOURS TOTAL)
ğŸ• MILESTONE 0 â€” FREEZE SCOPE (15 min)

Do NOT add new features beyond this list.

âœ… Shopify + generic sites only
âœ… Axios first, Playwright fallback (basic)
âŒ No proxy rotation
âŒ No ML classifiers
âŒ No UI polish

This is a backend engineering submission, not a scraping olympiad.

ğŸ• MILESTONE 1 â€” COMPANY AUTO-FILL (1.5 hours)
Objective

When user submits a website:

Company name

About

Email

Phones

Social links

Platform

What to implement (minimum but impressive)
1ï¸âƒ£ Keep your existing company.controller.js

BUT add:

/about, /contact deep page fetch

Data quality check

Playwright fallback only if needed

2ï¸âƒ£ Define â€œqualityâ€ clearly
name exists
AND about.length >= 50


If not â†’ fallback.

3ï¸âƒ£ Demo scenario

CakeToppers â†’ Axios works

Titan â†’ Axios fails â†’ Playwright works

This alone makes your project stand out.

ğŸ• MILESTONE 2 â€” PRODUCT SCRAPE BUTTON (1.5 hours)
Objective

User clicks button â†’ backend scrapes products.

What to do

Create ONE new API:

POST /api/company/:companyId/scrape-products


Controller steps:

Fetch company from DB

Run discoverUrls

Run classifyUrls

Run extractProducts

Run dbUpsert

Return count

âš ï¸ IMPORTANT
Reuse your existing pipeline code.
Do NOT rewrite discovery/extraction.

Demo

Trigger API

Products appear in MongoDB

products.json updated

ğŸ• MILESTONE 3 â€” WEEKLY AUTO UPDATE (1 hour)
Objective

Show engineering maturity, not complexity.

What to do

You already have:

cron.js

runPipeline

Just add:

comment: â€œWeekly schedule in productionâ€

set cron to weekly (for submission)

cron.schedule("0 2 * * 0", runPipeline);


Explain:

â€œRuns every Sunday at 2 AM to refresh productsâ€

No need to over-engineer diffing right now.

ğŸ• MILESTONE 4 â€” DATA SAFETY & FAIL-SAFE (45 min)
Add these QUICK wins
1ï¸âƒ£ Never crash

Wrap Axios

Wrap Playwright

Return null, not throw

2ï¸âƒ£ Scrape status

You already have:

scrapeStatus: "success" | "failed" | "partial"


Use it properly.

3ï¸âƒ£ Logs

Add:

console.log("[DISCOVERY]", ...)
console.log("[EXTRACT]", ...)
console.log("[UPSERT]", ...)


This matters a LOT in reviews.

ğŸ• MILESTONE 5 â€” README + STORY (45 min)

This is CRITICAL for submission.

README must include:
1ï¸âƒ£ Architecture diagram (ASCII is fine)
User â†’ API â†’ Scraper â†’ DB
             â†“
         Cron Scheduler

2ï¸âƒ£ Explain WHY

Axios first (fast)

Playwright fallback (JS-heavy sites)

Page caching (avoid refetch)

Shopify JSON (accuracy)

3ï¸âƒ£ Known limitations (IMPORTANT)

CAPTCHA sites

Aggressive bot protection

No proxy yet

Reviewers LOVE honesty.

ğŸ§  WHAT MAKES YOUR PRODUCT â€œBESTâ€ (EVEN IN 6 HOURS)

Not number of features.

But:

Clean architecture

Thoughtful fallbacks

Real-world constraints acknowledged

Working demo with real brands

You already have Titan + Bewakoof, which is impressive.

ğŸ“¦ FINAL SUBMISSION CHECKLIST

Before you submit, verify:

âœ… Company auto-fill works
âœ… Product scrape works via API
âœ… Cron exists and runs
âœ… MongoDB shows data
âœ… README explains decisions
âœ… Code is readable, not hacked

ğŸš€ IF YOU WANT, I CAN NEXT

Break each milestone into exact file edits

Help write submission explanation

Review your README

Simulate interviewer questions & answers

Just tell me what you want next.